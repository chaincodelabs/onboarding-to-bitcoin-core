= Consensus & validation

TIP: This section has been updated to Bitcoin Core @ https://github.com/bitcoin/bitcoin/tree/v23.0[v23.0^]

One of the fundamental concepts underlying bitcoin is that nodes on the network are able to maintain decentralized consensus on the ordering of transactions in the system.

The primary mechanism at work is that all nodes validate every block, and every transaction contained within that block, against their own copy of the consensus rules.
The secondary mechanism is that in the event of a discrepancy between two competing chain tips nodes should follow the chain with the most cumulative proof-of-work.
The result is that all honest nodes in the network will eventually converge onto a single, canonical, valid chain.

WARNING: If all nodes do not compute consensus values identically (including edge cases) a chainsplit will result.

For more information on how the bitcoin networks' decentralized consensus mechanism works see the Mastering Bitcoin section on https://github.com/bitcoinbook/bitcoinbook/tree/develop/ch10.asciidoc#decentralized-consensus[decentralized consensus^].

TIP: In Bitcoin Core there are an extra level of validation checks applied to incoming transactions in addition to consensus checks called "policy" which have a slightly different purpose, see <<Consensus vs Policy>> for more information on the differences between the two.

Consensus::
A collection of functions and variables which **must** be computed identically to all https://bitnodes.io/nodes/[other^] nodes on the network in order to remain in consensus and therefore on the main chain.

Validation::
Validation of blocks, transactions and scripts, with a view to permitting them to be added to either the blockchain (must pass consensus checks) or our local mempool (must pass policy checks).

== Consensus in Bitcoin Core

Naturally one might assume that all code related to consensus could be found in the _src/consensus/_ directory, however this is not entirely the case.
Components of consensus-related code can be found across the Bitcoin Core codebase in a number of files, including but not limited to:

[#consensus-components,listing]
----
ðŸ“‚ bitcoin
  ðŸ“‚ src
    ðŸ“‚ consensus
    ðŸ“‚ script
      ðŸ“„interpreter.cpp
    ðŸ“„ validation.h
    ðŸ“„ validation.cpp
----

Consensus-critical functions can also be found in proximity to code which could affect whether a node considers a transaction or block valid.
This could extend to, for example, block storage <<Database consensus,database>> code.

An abbreviated list of some of the more notable consensus functions and variables is shown below.

.Some consensus functions and variables
[cols="2,4"]
|===
|File |Objects

|_src/consensus/amount.h_
|`COIN`, `MAX_MONEY`, `MoneyRange()`

|_src/consensus/consensus.h_
|`BLOCK{SIZE\|WEIGHT\|SIGOPS_COST}`, `COINBASE_MATURITY`, `WITNESS_SCALE_FACTOR`, `MIN_TX_WEIGHT`

|_src/consensus/merkle.{h\|cpp}_
|`ComputeMerkleRoot(),` `BlockMerkleRoot(),` `BlockWitnessMerkleRoot()`

|_src/consensus/params.h_
|`BuriedDeployment`, `Params`(buried blocks which are valid but known to fail default script verify checks, BIP height activations, PoW params)

|_src/consensus/tx_check.{h\|cpp}_
|`CheckTransaction()`

|_src/consensus/tx_verify.{h\|cpp}_
|`CheckTxInputs(),` `Get{Legacy}SigOpCount()`, `IsFinalTx(),` `SequenceLock(s)()`

|_src/consensus/validation.h_
|`TxValidationResult` (validation result reason), `BlockValidationResult` (validation result reason), `ValidationState`, `Get{Transaction\|Block\|TransactionInput}Weight()`

|===

=== Consensus model

The consensus model in the codebase can be thought of as a database of the current state of the blockchain.
When a new block is learned about it is processed and the consensus code must determine which block is the current best.
Consensus can be thought of as a function of available information -- it's output is simply a deterministic function of its input.

There are a simple set of rules for determining the best block:

. Only consider valid blocks
. Where multiple chains exist choose the one with the most cumulative Proof of Work (PoW)
. If there is a tie-breaker (same height and work), then use first-seen

The result of these rules is a tree-like structure from genesis to the current day, building on only valid blocks.

Whilst this is easy-enough to reason about in theory, the implementation doesn't work exactly like that.
It must consider state, do I go forward or backwards for example.

== Validation in Bitcoin Core

Originally consensus and validation were much of the same thing, in the same source file.
However splitting of the code into strongly delineated sections was never fully completed, so validation.* files still hold some consensus codepaths.

== Consensus vs Policy

What is the difference between consensus and policy checks?
Both seem to be related to validating transactions.
We can learn a lot about the answer to this question from sdaftuar's StackExchange https://bitcoin.stackexchange.com/questions/100317/what-is-the-difference-between-policy-and-consensus-when-it-comes-to-a-bitcoin-c/100319#100319[answer^].

The answer teaches us that policy checks are a superset of validation checks --  that is to say that a transaction that passes policy checks has implicitly passed consensus checks too.
Nodes perform policy-level checks on all transactions they learn about before adding them to their local mempool.
Many of the policy checks contained in `policy` are called from inside `validation`, in the context of adding a new transaction to the mempool.

== Consensus and validation bugs

Consensus and validation bugs can arise both from inside the Bitcoin Core codebase itself, and from external dependencies.
Bitcoin wiki https://en.bitcoin.it/wiki/Common_Vulnerabilities_and_Exposures[lists^] some CVE and other Exposures.

=== OpenSSL consensus failure

Pieter Wuille https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-July/009697.html[disclosed^] the possibility of a consensus failure via usage of OpenSSL.
The issue was that the OpenSSL signature verification was accepting *multiple* signature serialization formats (for the same signature) as valid.
This effectively meant that a transactions' ID (txid) could be changed, because the signature contributes to the txid hash.

.Click to show the code comments related to pubkey signature parsing from _pubkey.cpp_
[%collapsible]
====

.src/pubkey.cpp
[source,cpp,options=nowrap]
----
/** This function is taken from the libsecp256k1 distribution and implements
 *  DER parsing for ECDSA signatures, while supporting an arbitrary subset of
 *  format violations.
 *
 *  Supported violations include negative integers, excessive padding, garbage
 *  at the end, and overly long length descriptors. This is safe to use in
 *  Bitcoin because since the activation of BIP66, signatures are verified to be
 *  strict DER before being passed to this module, and we know it supports all
 *  violations present in the blockchain before that point.
 */
int ecdsa_signature_parse_der_lax(const secp256k1_context* ctx, secp256k1_ecdsa_signature* sig, const unsigned char *input, size_t inputlen) {
    // ...
}
----
====

There were a few cases to consider:

. signature length descriptor malleation (extension to 5 bytes)
. third party malleation: signature may be slightly "tweaked" or padded
. third party malleation: negating the `S` value of the signature

In the length descriptor case there is a higher risk of causing a consensus-related chainsplit.
The sender can create a normal-length valid signature, but which uses a 5 byte length descriptor meaning that it might not be accepted by OpenSSL on all platforms.

TIP: Note that the sender can also "malleate" the signature whenever they like, by simply creating a new one, but this will be handled differently than a length-descriptor-extended signature.

In the second case, signature tweaking or padding, there is a lesser risk of causing a consensus-related chainsplit.
However the ability of third parties to tamper with valid transactions may open up off-chain attacks related to Bitcoin services or layers (e.g. Lightning) in the event that they are relying on txids to track transactions.

It is interesting to consider the order of the steps taken to fix this potential vulnerability:

. First the default policy in Bitcoin Core was altered (via `isStandard()`) to prevent the software from relaying or accepting into the mempool transactions with non-DER signature encodings. +
This was carried out in https://github.com/bitcoin/bitcoin/pull/2520[PR#2520^].
. Following the policy change, the strict encoding rules were later enforced by consensus in https://github.com/bitcoin/bitcoin/pull/5713[PR#5713^].

We can see the resulting flag in the script verification enum:

.src/script/interpreter.h
[source,cpp,options=nowrap]
----
// Passing a non-strict-DER signature or one with undefined hashtype to a checksig operation causes script failure.
// Evaluating a pubkey that is not (0x04 + 64 bytes) or (0x02 or 0x03 + 32 bytes) by checksig causes script failure.
// (not used or intended as a consensus rule).
SCRIPT_VERIFY_STRICTENC = (1U << 1),
----

.Expand to see where this flag is checked in _src/script/interpreter.cpp_
[%collapsible]
====

[source,cpp,options=nowrap]
----
bool CheckSignatureEncoding(const std::vector<unsigned char> &vchSig, unsigned int flags, ScriptError* serror) {
    // Empty signature. Not strictly DER encoded, but allowed to provide a
    // compact way to provide an invalid signature for use with CHECK(MULTI)SIG
    if (vchSig.size() == 0) {
        return true;
    }
    if ((flags & (SCRIPT_VERIFY_DERSIG | SCRIPT_VERIFY_LOW_S | SCRIPT_VERIFY_STRICTENC)) != 0 && !IsValidSignatureEncoding(vchSig)) {
        return set_error(serror, SCRIPT_ERR_SIG_DER);
    } else if ((flags & SCRIPT_VERIFY_LOW_S) != 0 && !IsLowDERSignature(vchSig, serror)) {
        // serror is set
        return false;
    } else if ((flags & SCRIPT_VERIFY_STRICTENC) != 0 && !IsDefinedHashtypeSignature(vchSig)) {
        return set_error(serror, SCRIPT_ERR_SIG_HASHTYPE);
    }
    return true;
}

bool static CheckPubKeyEncoding(const valtype &vchPubKey, unsigned int flags, const SigVersion &sigversion, ScriptError* serror) {
    if ((flags & SCRIPT_VERIFY_STRICTENC) != 0 && !IsCompressedOrUncompressedPubKey(vchPubKey)) {
        return set_error(serror, SCRIPT_ERR_PUBKEYTYPE);
    }
    // Only compressed keys are accepted in segwit
    if ((flags & SCRIPT_VERIFY_WITNESS_PUBKEYTYPE) != 0 && sigversion == SigVersion::WITNESS_V0 && !IsCompressedPubKey(vchPubKey)) {
        return set_error(serror, SCRIPT_ERR_WITNESS_PUBKEYTYPE);
    }
    return true;
}
----
====

[TIP]
====
Do you think this approach -- first altering policy, followed later by consensus -- made sense for implementing the changes needed to fix this consensus vulnerability?
Are there circumstances where it might not make sense?
====

Having OpenSSL as a consensus-critical dependency to the project was ultimately fixed in https://github.com/bitcoin/bitcoin/pull/6954[PR#6954^] which switched to using the in-house libsecp256k1 library (as a <<Subtrees,subtree>>) for signature verification.

=== Database consensus

Historically Bitcoin Core used Berkeley DB (BDB) for transaction and block indices.
In 2013 a migration to LevelDB for these indices was included with Bitcoin Core v0.8.
What developers at the time could not foresee was that nodes that were still using BDB, all pre 0.8 nodes, were silently consensus-bound by a relatively obscure BDB-specific database lock counter.

TIP: BDB required a configuration setting for the total number of locks available to the database.

Bitcoin Core was interpreting a failure to grab the required number of locks as equivalent to block validation failing.
This caused some BDB-using nodes to mark blocks created by LevelDB-using nodes as invalid and caused a consensus-level chain split.
https://github.com/bitcoin/bips/tree/master/bip-0050.mediawiki[BIP 50^] provides further explanation on this incident.

WARNING: Although database code is not in close proximity to the `/src/consensus` region of the codebase it was still able to induce a consensus bug.

BDB has caused other potentially-dangerous behaviour in the past.
Developer Greg Maxwell https://btctranscripts.com/greg-maxwell/2015-04-29-gmaxwell-bitcoin-selection-cryptography/#qa[describes^] in a Q&A how even the same versions of BDB running on the same system exhibited non-deterministic behaviour which might have been able to initiate chain re-orgs.

=== An inflation bug

This Bitcoin Core https://bitcoincore.org/en/2018/09/20/notice/[disclosure^] details a potential inflation bug.

It originated from trying to speed up transaction validation in `main.cpp#CheckTransaction()` which is now `consensus/tx_check.cpp#CheckTransaction()`, something which would in theory help speed up IBD (and less noticeably singular/block transaction validation).
The result in Bitcoin Core versions 0.15.x -> 0.16.2 was that a coin that was created in a previous block, could be spent twice in the same block by a miner, without the block being rejected by other Bitcoin Core nodes (of the aforementioned versions).

Whilst this bug originates from validation, it can certainly be described as a breach of consensus parameters.
In addition, nodes of version 0.14.x <= `node_version` >= 0.16.3 would reject inflation blocks, ultimately resulting in a chain split provided that miners existed using both inflation-resistant and inflation-permitting clients.

== Hard & Soft Forks

Before continuing with this section, ensure that you have a good understanding of what soft and hard forks are, and how they differ.
Some good resources to read up on this further are found in the table below.

.Hard and soft fork resources
[%autowidth.stretch]
|===
|Title |Resource |Link

|What is a soft fork, what is a hard fork, what are their differences?
|StackExchange
|https://bitcoin.stackexchange.com/questions/30817/what-is-a-soft-fork-what-is-a-hard-fork-what-are-their-differences[link^]

|Soft forks
|bitcoin.it/wiki
|https://en.bitcoin.it/wiki/Softfork[link^]

|Hard forks
|bitcoin.it/wiki
|https://en.bitcoin.it/wiki/Hardfork[link^]

|Soft fork activation
|Bitcoin Optech
|https://bitcoinops.org/en/topics/soft-fork-activation/[link^]

|List of consensus forks
|BitMex research
|https://blog.bitmex.com/bitcoins-consensus-forks/[link^]

|A taxonomy of forks (BIP99)
|BIP
|https://github.com/bitcoin/bips/blob/master/bip-0099.mediawiki[link^]

|Modern Soft Fork Activation
|bitcoin-dev mailing list
|https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-January/017547.html[link^]

|Chain splits and Resolutions
|BitcoinMagazine guest
|https://bitcoinmagazine.com/technical/guest-post-chain-splits-and-resolutions[link^]

|===

When making changes to Bitcoin Core its important to consider whether they could have any impact on the **consensus rules**, or the interpretation of those rules.
If they do, then the changes will end up being either a soft or hard fork, depending on the nature of the rule change.

WARNING: As <<Database consensus,described>>, certain Bitcoin Core components, such as the block database can also unwittingly introduce forking behaviour, even though they do not directly modify consensus rules.

Some of the components which are known to alter consensus behaviour, and should therefore be approached with caution, are listed in the section <<consensus-components,consensus components>>.

Changes are not made to consensus values or computations without extreme levels of review and necessity.
In contrast, changes such as refactoring can be (and are) made to areas of consensus code, when we can be sure that they will not alter consensus validation.

== Making forking changes

There is some debate around whether it's preferable to make changes via soft or hard fork.
Each technique has advantages and disadvantages.

.Hard vs soft forks for changes
[%autowidth]
|===
|Type |Advantages |Disadvantages

|Soft fork
a|

* Backwards compatible
* **Low risk of chain split** in worst case

a|

* Cannot change all values (e.g. block size, money supply)
* Might require clever programming tricks
* Might introduce "technical debt" and associated comprehension burden on reviewers and future programmers

|Hard fork
a|

* Can change any values you want (e.g. block size, money supply)
* Might be cleaner (code-wise) and therefore easier to reason about

a|

* Not backwards compatible
** Requires all nodes to upgrade in lock-step
* **High risk of chainsplit**
* We have no experience with them
* Other changes often required
* See bitcoincore.org for https://bitcoincore.org/en/2015/12/23/capacity-increases-faq/[more information^]

|===

== Upgrading consensus rules with soft forks

When soft-forking in new bitcoin consensus rules it is important to consider how old nodes will interpret the new rules.
For this reason the preferred method historically was to make something (e.g. an unused OPCODE which was to be repurposed) "non-standard" prior to the upgrade.
Making the opcode non-standard has the effect that transaction scripts using it will not be relayed by nodes using this policy.
Once the soft fork is activated policy is amended to make relaying transactions using this opcode standard policy again, so long as they comply with the ruleset of the soft fork.

.Soft forking marble statues
[sidebar]
****
An analogy might be to think of the current consensus ruleset like a big block of marble.
The current rules have already been carved out of it and eventually it will form into a complex statue.

As we soft fork new rules into bitcoin we are taking an un-touched area of the marble and carving something new out of it.
Importantly with soft forks we can only ever take parts of the marble _away_, so we must be considerate about what, where and how much we carve out for any upgrade.

There are parts of the statue currently untouched because they're reserved for future upgrades.
****

Using the analogy above, we could think of OP_NOP opcodes as unsculpted areas of marble.

TIP: Currently OP_NOP1 and OP_NOP4-NOP_NOP10 remain available for this.

Once the opcode has been made non-standard we can then sculpt the new rule from the marble and later re-standardize transactions using the opcode so long as they follow the new rule.

This makes sense from the perspective of an old, un-upgraded node who we are trying to remain in consensus with.
From their perspective they see an OP_NOP performing (like the name implies) nothing, but not marking the transaction as invalid.
After the soft fork they will _still_ see the (repurposed) OP_NOP apparently doing nothing but also not failing the transaction.

However from the perspective of the upgraded node they now have two possible evaluation paths for the OP_NOP: 1) Do nothing (for the success case) and 2) Fail evaluation (for the failure case).
This is summarized in the table below.

.Soft forking changes using OP_NOP opcodes
[%autowidth]
|===
| |Before soft fork |After soft fork

|Legacy node
|1) Nothing
|1) Nothing

|Upgraded Node
|1) Nothing
|1) Nothing (soft forked rule evaluation success) +
2) Mark transaction invalid (soft forked rule evaluation failure)

|===

You may notice here that there is still room for discrepancy; a miner who is not upgraded could possibly include transactions in a block which were valid according to legacy nodes, but invalid according to upgraded nodes.
If this miner had any significant hashpower this would be enough to initiate a chainsplit, as upgraded miners would not follow this tip.

.Selecting upgrade activation times
[sidebar]
****
Originally Satoshi used height-based upgrade points for activating soft forks.
The bitcoin network was so small and concentrated, and Satoshi could dictate the height quite easily, that this worked OK in that era.

After Satoshi left attempts were made to make the activation point a more predictable moment in _time_; with the intent on assisting engineers and services who relied on knowing when the upgrade was likely to activate (as wall time).
For this reason BIP16 and BIP30 were activated on a (block) timestamp, after miners had signalled readiness for the upgrade in their coinbase transactions.

The concept of miner activated soft forks (MASF) were invented with https://github.com/bitcoin/bips/blob/master/bip-0034.mediawiki[BIP34^] which said that every coinbase transaction needed to include the (block) height as the first item in its scriptSig, along with an increased block version number.
The block height requirement had the effect that no two coinbase transactions could have the same txid, which was previously possible (see https://blockstream.info/block/00000000000271a2dc26e7667f8419f2e15416dc6955e5a6c6cdf3f2574dd08e[1^] and https://blockstream.info/block/00000000000743f190a18c5577a3c2d2a1f610ae9601ac046a38084ccb7cd721[2^] for example).
The increased version number was accompanied by rules which https://github.com/bitcoin/bitcoin/pull/1526/commits[stipulated^] a form of miner readiness signalling, which could avoid a diktat from any individual about what time a particular upgrade should be activated.

[TIP]
====
The UTXO in the second of those two blocks, along with a second block also containing a duplicate coinbase txid have a https://github.com/bitcoin/bitcoin/commit/ab91bf39b7c11e9c86bb2043c24f0f377f1cf514[special carve-out^] in the code to enable them to pass validation.

Unfortunately though the second UTXO effectively overwrote the first in the UTXO set, so in both cases 50 BTC was lost from the spendable supply.
====

MASF was used for BIP65 and BIP66.
A summary of the mechanism is:

* If 750/1000 blocks signal this new version number then the new rule is active.
* At 950/1000 you *must* signal.
** Forcibly kick the last 5% stragglers out.

However, even using miner signalling for BIP16 had already caused drama, as the idea of activation based on miner signalling was interpreted as a vote (by only miners), rather than what it was, which was miners saying "yes, I am ready for the upgrade".

When upgrading via soft fork we want everyone to be on the same page to minimize the risk of a chainsplit and miner signalling was deemed the best method we had to achieve rough consensus on this.

[quote,Eric Lombrozo, Bitcoin Magazine]
____
Whenever we want to change the consensus rules, this presents a serious problem because we donâ€™t really want to just force new rules on the network. Thereâ€™s no central authority that can do this really. We need to have a way for the network to adapt to the new rules, decide whether or not it wants to adjust to these rules, and to make sure that everyone still ends up agreeing in the end.
____

In the end bitcoin developers concluded that MASF indeed had potential for centralization and so produced the https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki[BIP9^] specification with which to use for future upgrades.

****

Repurposing OP_NOPs does have its limitations.
First and foremost they cannot manipulate the stack, as this is something that un-upgraded nodes would not expect or validate identically.
Getting rid of the OP_DROP requirement when using repurposed OP_NOPs would require a hard fork.

Examples of soft forks which re-purposed OP_NOPs include CLTV and CSV.
Ideally these operations would remove the subsequent object from the stack when they had finished processing it, so you will often see them followed by OP_DROP which removes the object, for example in the script used for the `to_local` output in a lightning commitment transaction:

.Lightning https://github.com/lightning/bolts/blob/master/03-transactions.md#to_local-output[commitment transaction^] output
[source,text,highlight=6;7]
----
OP_IF
    # Penalty transaction
    <revocationpubkey>
OP_ELSE
    `to_self_delay`
    OP_CHECKSEQUENCEVERIFY
    OP_DROP
    <local_delayedpubkey>
OP_ENDIF
OP_CHECKSIG
----

There are other limitations associated with repurposing OP_NOPs, and ideally bitcoin needed a better upgrade system...

=== SegWit upgrade

SegWit was the first attempt to go beyond simply repurposing OP_NOPs for upgrades.
The idea was that the `scriptPubKey`/`redeemScript` would consist of a 1 byte push opcode (0-16) followed by a data push between 2 and 40 bytes.
The value of the first push would represent the version number, and the second push the https://github.com/bitcoin/bips/blob/master/bip-0141.mediawiki#witness-program[witness program^].
If the conditions to interpret this as a SegWit script were matched, then this would be followed by a `witness`, whose data varied on whether this was a P2WPKH or P2WSH witness program.

Legacy nodes, who would not have the witness data, would interpret this output as `anyonecanspend` and so would be happy to validate it, whereas upgraded nodes could validate it using the additional `witness` against the new rules.
To revert to the statue analogy this gave us the ability to work with a new area of the marble which was entirely untouched.

The addition of a versioning scheme to SegWit was a relatively late addition which stemmed from noticing that, due to the CLEANSTACK policy rule which required exactly 1 true element to remain on the stack after execution, SegWit outputs would be of the form `OP_N + DATA`.
With SegWit we wanted a compact way of creating a new output which didn't have any consensus rules associated with it, yet had lots of freedom, was ideally already non-standard, and was permitted by CLEANSTACK.

The solution was to use two pushes: according to old nodes there are two elements, which was non-standard.
The first push must be at least one byte, so we can use one of the `OP_N` opcodes, which we then interpret as the SegWit version.
The second is the data we have to push.

Whilst this immediately gave us new upgrade paths via SegWit versions Taproot (SegWit version 1) went a step further and declared _new opcodes inside of SegWit_, also evaluated as `anyonecanspend` by nodes that don't support SegWit, giving us yet more soft fork upgradability.
These opcodes could in theory be used for anything, for example if there was ever a need to have a new consensus rule on 64 bit numbers we could use one of these opcodes.

== Fork wish lists

There are a number of items that developers have had on their wish lists to tidy up in future fork events.

An https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-March/016714.html[email^] from Matt Corallo with the subject "The Great Consensus Cleanup" described a "wish list" of items developers were keen to tidy up in a future soft fork.

The Hard Fork Wishlist is described on this en.bitcoin.it/wiki https://en.bitcoin.it/wiki/Hardfork_Wishlist[page^].
The rationale for collecting these changes together, is that if backwards-incompatible (hard forking) changes are being made, then we "might as well" try and get a few in at once, as these events are so rare.

== Bitcoin core consensus specification

A common question is where the bitcoin protocol is documented, i.e. specified.
However bitcoin does not have a formal specification, even though many ideas have some specification (in <<BIPs>>) to aid re-implementation.

IMPORTANT: The requirements to be compliant with "the bitcoin spec" are to be bug-for-bug compatible with the Bitcoin Core implementation.

The reasons for Bitcoin not having a codified specification are historical; Satoshi never released one.
Instead, in true "Cypherpunks write code" style and after releasing a general whitepaper, they simply released the first client.
This client existed on it's own for the best part of two years before others sought to re-implement the rule-set in other clients:

* https://github.com/libbitcoin/libbitcoin-system/commit/9dea4682bf0e4247f3c4cb8a6c140ade61bf7df7[libbitcoin^]
* https://github.com/bitcoinj/bitcoinj/commit/d1036b101f01b7ab79fc3e10e5199f80f478674d[BitcoinJ^]

A forum https://bitcointalk.org/index.php?topic=195.msg1611#msg1611[post^] from Satoshi in June 2010 had however previously discouraged alternative implementations with the rationale:

[quote,Satoshi Nakamoto]
____
...

I don't believe a second, compatible implementation of Bitcoin will ever be a good idea.  So much of the design depends on all nodes getting exactly identical results in lockstep that a second implementation would be a menace to the network.  The MIT license is compatible with all other licenses and commercial uses, so there is no need to rewrite it from a licensing standpoint.
____

It is still a point of contention amongst some developers in the community, however the fact remains that if you wish to remain in consensus with the majority of (Bitcoin Core) nodes on the network, you must be _exactly_ bug-for-bug compatible with Bitcoin Core's consensus code.

TIP: If Satoshi _had_ launched Bitcoin by providing a specification, could it have ever been specified well-enough to enable us to have multiple node implementations?

[TIP]
====
One mechanism often employed by those who want to run custom node software is to position an up-to-date Bitcoin Core node to act as a "gateway" to the network.
Internally your own node can then make a single connection to this Bitcoin Core node.
This means that your custom internal node will now only receive transactions and blocks which have passed Bitcoin Core's consensus (or policy) checks, allowing you to be sure that your custom node is not accepting objects which could cause you to split onto a different chain tip.
====

== libbitcoinconsensus

The libbitcoinconsensus library is described in the 0.10.0 release notes:

[quote]
____
Consensus library

Starting from 0.10.0, the Bitcoin Core distribution includes a consensus library.

The purpose of this library is to make the verification functionality that is
critical to Bitcoin's consensus available to other applications, e.g. to language
bindings such as [python-bitcoinlib](https://pypi.python.org/pypi/python-bitcoinlib) or
alternative node implementations.

This library is called `libbitcoinconsensus.so` (or, `.dll` for Windows).
Its interface is defined in the C header [bitcoinconsensus.h](https://github.com/bitcoin/bitcoin/blob/0.10/src/script/bitcoinconsensus.h).

In its initial version the API includes two functions:

- `bitcoinconsensus_verify_script` verifies a script. It returns whether the indicated input of the provided serialized transaction
correctly spends the passed scriptPubKey under additional constraints indicated by flags
- `bitcoinconsensus_version` returns the API version, currently at an experimental `0`

The functionality is planned to be extended to e.g. UTXO management in upcoming releases, but the interface
for existing methods should remain stable.
____

== libbitcoinkernel

The https://github.com/bitcoin/bitcoin/issues/24303[libbitcoinkernel^] project seeks to modularise Bitcoin Cores' consensus engine and make it easier for developers to reason about when they are modifying code which could be consensus-critical.

This project differs from `libbitcoinconsensus` in that it is designed to be a stateful engine, with a view to eventually: being able to spawn its own threads, do caching (e.g. of script and signature verification), do its own I/O, and manage dynamic objects like a mempool.
Another benefit of fully extracting the consensus engine in this way may be that it becomes easier to write and reason about consensus test cases.

In the future, if a full de-coupling is successfully completed, other Bitcoin applications might be able to use `libbitcoinkernel` as their own consensus engine permitting multiple full node implementations to operate on the network in a somewhat safer manner than many of them operate under today.
The initial objective of this library however is to actually have it used by Bitcoin Core internally, something which is not possible with libbitcoinconsensus due to it's lack of caching and state (making it too slow to use).

The implementation approaches of libbitcoinconsensus and libbitcoinkernel also differ; with lb-consensus parts of consensus were moved into the library piece by piece, with the eventual goal that it would be encapsulated.
lb-kernel takes a different approach -- first cast a super wide net around everything needed to run a consensus engine, and then gradually strip pieces out where they can be.
In theory this should get us something which Bitcoin Core can use much faster (in fact, you can build the optional `bitcoin-chainstate` binary which already has some functionality).

Part of libbitcoinkernel has been merged in via Carl Dong's https://github.com/bitcoin/bitcoin/pull/24304[`bitcoin-chainstate` PR^].
It also has its own project https://github.com/bitcoin/bitcoin/projects/18[board^] to track progress.

== Hardcoded consensus values

_consensus/consensus.h_ contains a number of `static const` values relating to consensus rules.
These are globally shared between files such as _validation.cpp_, _rpc_mining.cpp_ and _rpc/mining.cpp_.
These consensus-critical values are marked as `const` so that there is no possibility that they can be changed at any point during program execution.

One example of this would be the maximum block weight which should not ever be exceeded:

[source,cpp]
----
static const unsigned int MAX_BLOCK_WEIGHT = 4000000;
----

_consensus/amount.h_ contains the conversion rate between satoshis and one "bitcoin", as well as a `MAX_MONEY` constant.
These are marked as `constexpr` to indicate that they should be evaluated at compile time and then remain as `const` during execution.

[source,cpp]
----
/** The amount of satoshis in one BTC. */
static constexpr CAmount COIN = 100000000;

/** No amount larger than this (in satoshi) is valid.
 *
 * Note that this constant is *not* the total money supply, which in Bitcoin
 * currently happens to be less than 21,000,000 BTC for various reasons, but
 * rather a sanity check. As this sanity check is used by consensus-critical
 * validation code, the exact value of the MAX_MONEY constant is consensus
 * critical; in unusual circumstances like a(nother) overflow bug that allowed
 * for the creation of coins out of thin air modification could lead to a fork.
 * */
static constexpr CAmount MAX_MONEY = 21000000 * COIN;
----

[TIP]
====
Do you think that the `COIN` constant is necessary at a consensus level, or is it a Bitcoin Core-specific abstraction?
====

== Transaction validation

Transactions can originate from the P2P network, the wallet, RPCs or from tests.

Transactions which originate from the wallet, RPCs or individually from the P2P network (from a `NetMsgType::TX` message) will follow a validation pathway which includes adding them to the mempool.
This implies passing both consensus and policy checks.
See the sections on <<Single transactions>> and <<multiple_transactions,Multiple transactions>> to learn more about transaction validation via the mempool.

Transactions which are learned about in a new block from the P2P network (from a `NetMsgType::BLOCK` or `NetMsgType::BLOCKTXN` message) do not have to be added to the mempool and so do not have to pass policy checks.
See the section <<Transactions from blocks>> to learn more about transaction validation bypassing the mempool.

.Transaction origination (excluding tests)
[mermaid,target=tx-origination,format=svg,id=tx-origination]
....
flowchart LR
    process_tx["ChainstateManager::ProcessTransaction()"]
    process_msg["PeerManagerImpl::ProcessMessage()"]
    process_block["ProcessBlock()"]
    check_block["CheckBlock()"]
    connect_block["ConnectBlock()"]
    process_orphan["PeerManagerImpl::ProcessOrphanTx()"]
    broadcast_tx["BroadcastTransaction()"]
    srt["sendrawtransaction()"]
    tmpa["testmempoolaccept()"]
    submit_relay["CWallet::SubmitTxMemoryPoolAndRelay()"]
    atmp["AcceptToMemoryPool()"]
    accept_single["AcceptSingleTransaction()"]
    process_package["ProcessNewPackage()"]
    accept_package["AcceptPackage()"]
    accept_multiple["AcceptMultipleTransactions()"]

    subgraph net_processing.cpp
        process_msg
        process_orphan
    end
    subgraph 2 ["rpc/rawtransaction.cpp"]
        srt
        tmpa
    end
    subgraph 3 [wallet/wallet.cpp]
        submit_relay
    end
    process_msg -. Packages ..-> process_package
    process_msg ---> process_block --> check_block --> connect_block
    process_msg -- NetMessage::TX --> process_tx
    process_orphan ---> process_tx
    tmpa --> process_tx
    submit_relay --> broadcast_tx
    srt --> broadcast_tx
    broadcast_tx --> process_tx
    process_tx --> atmp --> accept_single
    srt -- Packages --> process_package
    process_package --> accept_package
    accept_package --> accept_multiple

    classDef P2P fill:red,color:white,stroke:red;
    classDef Wallet fill:green,color:white,stroke:green;
    classDef RPC fill:blue,color:white,stroke:blue;
    class process_msg,process_orphan P2P
    class submit_relay Wallet
    class tmpa,srt RPC
....

NOTE: Dotted lines represent potential future upgrades

[NOTE]
====
P2P network = [red]#Red# +
Wallet = [green]#Green# +
RPCs = [blue]#Blue#
====

TIP: For more information on `PeerManagerImpl` see <<pimpl-technique,PIMPL technique>> in the appendix.

Transactions are internally represented as either a `CTransaction`, a `CTransactionRef` (a shared pointer to a `CTransaction`) or in the case of packages a `Package` which is a `std::vector<CTransactionRef>`.

We can follow the journey of a transaction through the Bitcoin Core mempool by following glozow's https://github.com/glozow/bitcoin-notes/tree/e9855dc377811b6d77bb75d8606c776cc26c1860/transaction-lifecycle.md#Validation-and-Submission-to-Mempool[notes^] on transaction "Validation and Submission to the Mempool".
glozow details the different types of checks that are run on a new transaction before it's accepted into the mempool, as well as breaking down how these checks are different from each other: consensus vs policy, script vs non-script, contextual vs context-free.

The section on block validation https://github.com/glozow/bitcoin-notes/tree/e9855dc377811b6d77bb75d8606c776cc26c1860/transaction-lifecycle.md#block-validation[describes^] the consensus checks performed on newly-learned blocks, specifically:

[quote,glozow]
____
Since v0.8, Bitcoin Core nodes have used a https://github.com/bitcoin/bitcoin/pull/1677[UTXO set^] rather than blockchain lookups to represent state and validate transactions.
To fully validate new blocks nodes only need to consult their UTXO set and knowledge of the current consensus rules.
Since consensus rules depend on block height and time (both of which can *decrease* during a reorg), they are recalculated for each block prior to validation.

Regardless of whether or not transactions have already been previously validated and accepted to the mempool, nodes check block-wide consensus rules (e.g. https://github.com/bitcoin/bitcoin/tree/9df1906091f84d9a3a2e953a0424a88e0931ea33/src/validation.cpp#L1935[total sigop cost^], https://github.com/bitcoin/bitcoin/blob/9df1906091f84d9a3a2e953a0424a88e0931ea33/src/validation.cpp#L1778-L1866[duplicate transactions^], https://github.com/bitcoin/bitcoin/blob/9df1906091f84d9a3a2e953a0424a88e0931ea33/src/validation.cpp#L3172-L3179[timestamps^], https://github.com/bitcoin/bitcoin/blob/9df1906091f84d9a3a2e953a0424a88e0931ea33/src/validation.cpp#L3229-L3255[witness commitments^] https://github.com/bitcoin/bitcoin/blob/9df1906091f84d9a3a2e953a0424a88e0931ea33/src/validation.cpp#L1965-L1969[block subsidy amount^]) and transaction-wide consensus rules (e.g. availability of inputs, locktimes, and https://github.com/bitcoin/bitcoin/blob/9df1906091f84d9a3a2e953a0424a88e0931ea33/src/validation.cpp#L1946[input scripts^]) for each block.

Script checking is parallelized in block validation. Block transactions are checked in order (and coins set updated which allows for dependencies within the block), but input script checks are parallelizable. They are added to a https://github.com/bitcoin/bitcoin/tree/9df1906091f84d9a3a2e953a0424a88e0931ea33/src/validation.cpp#L1887[work queue^] delegated to a set of threads while the main validation thread is working on other things.
While failures should be rare - creating a valid proof of work for an invalid block is quite expensive - any consensus failure on a transaction invalidates the entire block, so no state changes are saved until these threads successfully complete.

If the node already validated a transaction before it was included in a block, no consensus rules have changed, and the script cache has not evicted this transaction's entry, it doesn't need to run script checks again - it just https://github.com/bitcoin/bitcoin/tree/1a369f006fd0bec373b95001ed84b480e852f191/src/validation.cpp#L1419-L1430[uses the script cache^]!
____

The section from bitcoin-core-architecture on script verification also https://github.com/chaincodelabs/bitcoin-core-onboarding/tree/main/1.0_bitcoin_core_architecture.asciidoc#script-verification[highlights^] how the script interpreter is called from at least 3 distinct sites within the codebase:

[quote]
____
* when the node https://github.com/bitcoin/bitcoin/tree/4b5659c6b115315c9fd2902b4edd4b960a5e066e/src/net_processing.cpp#L3001[receives a new transaction^].

* when the https://github.com/bitcoin/bitcoin/tree/4b5659c6b115315c9fd2902b4edd4b960a5e066e/src/node/transaction.cpp#L29[node wants to broadcast a new transaction^].

* when https://github.com/bitcoin/bitcoin/tree/4b5659c6b115315c9fd2902b4edd4b960a5e066e/src/net_processing.cpp#L3529[receiving a new block^]
____

Having considered both transactions that have entered into the mempool and transactions that were learned about in a new block we now understand both ways a transaction can be considered for validation.

TIP: As you read through the following sub-sections, consider whether making changes to them could affect policy or consensus.

=== Single transactions

`AcceptToMemoryPool()` (ATMP) is where the checks on single transactions occur before they enter the mempool.

.ATMP validation flow chart
[mermaid,target=ATMP-validation-flow,format=svg,id=ATMP-validation-flow]
....
flowchart TB
    success[MempoolAcceptResult::Success]
    failure[MempoolAcceptResult::Failure]
    process_tx["ChainstateManager::ProcessTransaction()"]
    maybe_update["CChainState::MaybeUpdateMempoolForReorg()"]
    load_mempool["LoadMempool()"]
    atmp["AcceptToMemoryPool()"]
    accept_single["AcceptSingleTransaction()"]
    finalise["Finalize()"]

    %% think this is too much detail
    %% process_msg["PeerManagerImpl::ProcessMessage()"]
    %% process_orphan["PeerManagerImpl::ProcessOrphanTx()"]
    %% broadcast_tx["BroadcastTransaction()"]
    %% process_msg --> process_tx
    %% process_orphan --> process_tx
    %% broadcast_tx --> process_tx

    maybe_update --> atmp
    process_tx --> atmp
    load_mempool --> atmp

    atmp --> accept_single
    accept_single --> PreChecks
    PreChecks --> ReplacementChecks
    PreChecks -- fail --> failure
    ReplacementChecks --> PolicyScriptChecks
    ReplacementChecks -- fail --> failure
    PolicyScriptChecks --> ConsensusScriptChecks
    PolicyScriptChecks -- fail --> failure
    ConsensusScriptChecks -- if test_accept --> success
    ConsensusScriptChecks -- fail ---> failure
    ConsensusScriptChecks --> finalise
    finalise --> success

    classDef green fill:#00A000,color:white,stroke:green;
    classDef red fill:#BA3925,color:white,stroke:red;
    class AcceptToMemoryPool,success green
    class failure, red
....

You can see the calls to the various `*Checks()` functions in the <<ATMP-validation-flow,call graph>>, and the order in which they are run.

Let's take a look inside ``AcceptToMemoryPool()``'s inner function `AcceptSingleTransaction()` which handles running the checks:

.src/validation.cpp
[source,cpp,highlight=8;10;14;16,options=nowrap,id=accept_single_transaction]
----
MempoolAcceptResult MemPoolAccept::AcceptSingleTransaction(const CTransactionRef& ptx, ATMPArgs& args)
{
    AssertLockHeld(cs_main);
    LOCK(m_pool.cs); // mempool "read lock" (held through GetMainSignals().TransactionAddedToMempool())

    Workspace ws(ptx);

    if (!PreChecks(args, ws)) return MempoolAcceptResult::Failure(ws.m_state);

    if (m_rbf && !ReplacementChecks(ws)) return MempoolAcceptResult::Failure(ws.m_state);

    // Perform the inexpensive checks first and avoid hashing and signature verification unless
    // those checks pass, to mitigate CPU exhaustion denial-of-service attacks.
    if (!PolicyScriptChecks(args, ws)) return MempoolAcceptResult::Failure(ws.m_state);

    if (!ConsensusScriptChecks(args, ws)) return MempoolAcceptResult::Failure(ws.m_state);

    // Tx was accepted, but not added
    if (args.m_test_accept) {
        return MempoolAcceptResult::Success(std::move(ws.m_replaced_transactions), ws.m_vsize, ws.m_base_fees);
    }

    if (!Finalize(args, ws)) return MempoolAcceptResult::Failure(ws.m_state);

    GetMainSignals().TransactionAddedToMempool(ptx, m_pool.GetAndIncrementSequence());

    return MempoolAcceptResult::Success(std::move(ws.m_replaced_transactions), ws.m_vsize, ws.m_base_fees);
}
----

[TIP]
====
We purposefully run checks in this order so that the least computationally-expensive checks are run first.
This means that we can hopefully fail early and minimise CPU cycles used on invalid transactions.
====

WARNING: If an attacker could force us to perform many expensive computations simply by sending us many invalid transactions then it would be inexpensive to bring our node to a halt.

Once `AcceptSingleTransaction` has acquired the `cs_main` and `m_pool.cs` locks it initializes a `Workspace` struct -- a storage area for (validation status) state which can be shared by the different validation checks.
Caching this state avoids performing the same computations multiple times and is important for performance.
It will pass this workspace, along with the struct of `ATMPArgs` it received as argument, to the checks.

.Click to see the code comments on why we hold _two_ locks before performing consensus checks on transactions
[%collapsible,id=multiple_locks]
====
.src/txmempool.h#CTxMemPool
[source,cpp,options=nowrap]
----
/**
 * This mutex needs to be locked when accessing `mapTx` or other members
 * that are guarded by it.
 *
 * @par Consistency guarantees
 *
 * By design, it is guaranteed that:
 *
 * 1. Locking both `cs_main` and `mempool.cs` will give a view of mempool
 *    that is consistent with current chain tip (`::ChainActive()` and
 *    `CoinsTip()`) and is fully populated. Fully populated means that if the
 *    current active chain is missing transactions that were present in a
 *    previously active chain, all the missing transactions will have been
 *    re-added to the mempool and should be present if they meet size and
 *    consistency constraints.
 *
 * 2. Locking `mempool.cs` without `cs_main` will give a view of a mempool
 *    consistent with some chain that was active since `cs_main` was last
 *    locked, and that is fully populated as described above. It is ok for
 *    code that only needs to query or remove transactions from the mempool
 *    to lock just `mempool.cs` without `cs_main`.
 *
 * To provide these guarantees, it is necessary to lock both `cs_main` and
 * `mempool.cs` whenever adding transactions to the mempool and whenever
 * changing the chain tip. It's necessary to keep both mutexes locked until
 * the mempool is consistent with the new chain tip and fully populated.
 */
mutable RecursiveMutex cs;
----
====

The `Workspace` is initialized with a pointer to the transaction (as a `CTransactionRef`) and holds some https://github.com/bitcoin/bitcoin/blob/v23.0/src/validation.cpp#L553-L593[additional^] information related to intermediate state.

We can look at the https://github.com/bitcoin/bitcoin/blob/v23.0/src/validation.cpp#L469-L534[`ATMPArgs` struct^] to see what other information our mempool wants to know about in addition to transaction information.

.ATMPArgs
[sidebar]
****
`m_accept_time` is the local time when the transaction entered the mempool.
It's used during the mempool transaction eviction selection process as part of `CTxMemPool::Expire()` where it is referenced by the name `entry_time`:

.Click to see `entry_time` being used in `Expire()`
[%collapsible]
====
.src/txmempool.cpp#CTXMemPool::Expire()
[source,cpp,highlight=4;6,options=nowrap]
----
int CTxMemPool::Expire(std::chrono::seconds time)
{
    AssertLockHeld(cs);
    indexed_transaction_set::index<entry_time>::type::iterator it = mapTx.get<entry_time>().begin();
    setEntries toremove;
    while (it != mapTx.get<entry_time>().end() && it->GetTime() < time) {
        toremove.insert(mapTx.project<0>(it));
        it++;
    }
    setEntries stage;
    for (txiter removeit : toremove) {
        CalculateDescendants(removeit, stage);
    }
    RemoveStaged(stage, false, MemPoolRemovalReason::EXPIRY);
    return stage.size();
}
----
====

`m_bypass_limits` is used to determine whether we should enforce mempool fee limits for this transaction.
If we are a miner we may want to ensure _our own_ transactions would pass mempool checks, even if we don't attach a fee to them.

`m_test_accept` is used if we just want to run mempool checks to test validity, but not actually add the transaction into the mempool yet.
This happens when we want to broadcast one of our own transactions, done by calling `BroadcastTransaction` from `node/transaction.cpp#BroadcastTransaction()` or from the `testmempoolaccept()` RPC.
****

If all the checks pass and this was not a `test_accept` submission then we will `MemPoolAccept::Finalize` the transaction, adding it to the mempool, before trimming the mempool size and updating any affected RBF transactions as required.

[#multiple_transactions]
=== Multiple transactions (and packages)

TODO: This section should start from `AcceptPackage()` and flow through from there, including `AcceptMultipleTransactions()` as a sub-section.

It's possible to consider multiple transactions for validation together, via `AcceptMultipleTransactions()` found in _src/net_processing.cpp_.
It's currently only available from tests (`test/tx_package_tests.cpp`) and the `testmempoolaccept` RPC (via `ProcessNewPackage()`), but the intention is for it to be available to packages received from the P2P network in the future.

This validation flow has been created for usage with Package Mempool Accept, which glozow has written up in a https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a[gist^] (https://archive.ph/Uhewe[archive^]).

The flow here is similar to <<accept_single_transaction, `AcceptSingleTransaction()`>> in that we start by grabbing `cs_main` before initializing validation state and workspaces, however this time we use `PackageValidationState` and a vector of workspaces, `std::vector<Workspace>`.
Each transaction therefore has it's own workspace but all transactions in the package share a single validation state.
This aligns with the goal of either accepting or rejecting the entire package as a single entity.

Next come two `for` loops over the vector of workspaces (i.e. transactions).
The first performs the <<PreChecks,`PreChecks()`>>, but this time also freeing up coins to be spent by other transactions in this package.
This would not usually be possible (nor make sense) _within_ an `AcceptTransaction()` flow, but within a package we want to be able to validate transactions who use as inputs, other transactions not yet added to our mempool:

[source,cpp,options=nowrap]
----
    // Make the coins created by this transaction available for subsequent transactions in the
    // package to spend. Since we already checked conflicts in the package and we don't allow
    // replacements, we don't need to track the coins spent. Note that this logic will need to be
    // updated if package replace-by-fee is allowed in the future.
    assert(!args.m_allow_bip125_replacement);
    m_viewmempool.PackageAddTransaction(ws.m_ptx);
----

If the `PreChecks` do not fail, we call `m_viewmempool.PackageAddTransaction()` passing in the workspace.
This adds the transaction to a map in our Mempool called `std::unordered_map<COutPoint, Coin, SaltedOutpointHasher> m_temp_added;`, which is essentially a temporary cache somewhere in-between being validated and being fully added to the mempool.

TODO: Fix after adding section on `AcceptPackage`

After this first loop we perform `PackageMempoolChecks()` which first asserts that transactions are not already in the mempool, before checking the "PackageLimits".

=== PreChecks

The code comments for `PreChecks` give a clear description of what the PreChecks are for:

.src/validation.cpp#MemPoolAccept::PreChecks()
[source,cpp,options=nowrap]
----
// Run the policy checks on a given transaction, excluding any script checks.
// Looks up inputs, calculates feerate, considers replacement, evaluates
// package limits, etc. As this function can be invoked for "free" by a peer,
// only tests that are fast should be done here (to avoid CPU DoS).
----

The `PreChecks` function is very https://github.com/bitcoin/bitcoin/blob/v23.0/src/validation.cpp#L668-L906[long^] but is worth examining to understand better which checks are undertaken as part of this first stage.

=== ReplacementChecks

During `PreChecks` the `m_rbf` bool will have been set to `true` if it is determined that this transaction would have to replace an existing transaction from our mempool.
If this bool is set, then `ReplacementChecks` will be run.
These checks are designed to check that BIP125 RBF rules are being adhered to.

=== PolicyScriptChecks

Following `ReplacementChecks` we initialise a `PrecomputedTransactionData` struct in the `Workspace` which will hold expensive-to-compute data that we might want to use again in subsequent validation steps.

.Click to show the `PrecomputedTransactionData` struct
[%collapsible]
====
.script/interpreter.cpp
[source,cpp,options=nowrap]
----
struct PrecomputedTransactionData
{
    // BIP341 precomputed data.
    // These are single-SHA256, see https://github.com/bitcoin/bips/blob/master/bip-0341.mediawiki#cite_note-15.
    uint256 m_prevouts_single_hash;
    uint256 m_sequences_single_hash;
    uint256 m_outputs_single_hash;
    uint256 m_spent_amounts_single_hash;
    uint256 m_spent_scripts_single_hash;
    //! Whether the 5 fields above are initialized.
    bool m_bip341_taproot_ready = false;

    // BIP143 precomputed data (double-SHA256).
    uint256 hashPrevouts, hashSequence, hashOutputs;
    //! Whether the 3 fields above are initialized.
    bool m_bip143_segwit_ready = false;

    std::vector<CTxOut> m_spent_outputs;
    //! Whether m_spent_outputs is initialized.
    bool m_spent_outputs_ready = false;

    PrecomputedTransactionData() = default;

    template <class T>
    void Init(const T& tx, std::vector<CTxOut>&& spent_outputs);

    template <class T>
    explicit PrecomputedTransactionData(const T& tx);
};
----
====

Next we call `PolicyScriptChecks()` passing in the same `ATMPArgs` and `Workspace` that we used with PreChecks.
This is going to check the transaction against our individual node's policies.

[TIP]
====
Note that local node policies are not necessarily consensus-binding, but are designed to help prevent resource exhaustion (e.g. DoS) on our node.

See the <<Transaction validation>> and <<Consensus in Bitcoin Core>> sections for more information on the differences between policy and consensus.
====

`PolicyScriptChecks()` starts with initialisation of the transaction into a `CTransaction`, before beginning to https://github.com/bitcoin/bitcoin/blob/v23.0/src/validation.cpp#L973-L999[check^] the input scripts against the script flags.

.src/validation.cpp#PolicyScriptChecks
[source,cpp,options=nowrap]
----
bool MemPoolAccept::PolicyScriptChecks(const ATMPArgs& args, Workspace& ws, PrecomputedTransactionData& txdata)
{
    const CTransaction& tx = *ws.m_ptx;
    TxValidationState& state = ws.m_state;

    constexpr unsigned int scriptVerifyFlags = STANDARD_SCRIPT_VERIFY_FLAGS;

    // Check input scripts and signatures.
    // This is done last to help prevent CPU exhaustion denial-of-service attacks.
    if (!CheckInputScripts(tx, state, m_view, scriptVerifyFlags, true, false, txdata)) { <1>
        // SCRIPT_VERIFY_CLEANSTACK requires SCRIPT_VERIFY_WITNESS, so we
        // need to turn both off, and compare against just turning off CLEANSTACK
        // to see if the failure is specifically due to witness validation.
        TxValidationState state_dummy; // Want reported failures to be from first CheckInputScripts
        if (!tx.HasWitness() && CheckInputScripts(tx, state_dummy, m_view, scriptVerifyFlags & ~(SCRIPT_VERIFY_WITNESS | SCRIPT_VERIFY_CLEANSTACK), true, false, txdata) &&
                !CheckInputScripts(tx, state_dummy, m_view, scriptVerifyFlags & ~SCRIPT_VERIFY_CLEANSTACK, true, false, txdata)) {
            // Only the witness is missing, so the transaction itself may be fine.
            state.Invalid(TxValidationResult::TX_WITNESS_STRIPPED,
                    state.GetRejectReason(), state.GetDebugMessage());
        }
        return false; // state filled in by CheckInputScripts
    }

    return true;
}
----

<1> Calling `CheckInputScripts()` involves ECDSA signature verification and is therefore computationally expensive.

// TODO: Why?
If the script type is SegWit an additional round of checking is performed, this time including the `CLEANSTACK` rule.
The call(s) flag `cacheSigStore` as `true`, and `cacheFullScriptStore` as `false`, which means that matched signatures will be persisted in the cache, but matched full scripts will be removed.

=== ConsensusScriptChecks

If the `PolicyScriptChecks` return `true` we will move on to consensus script checks, again passing in the same `ATMPArgs`, `Workspace` and now `PrecomputedTransactionData` that we used previously with `PolicyScriptChecks`.

The main check in here is `CheckInputsFromMempoolAndCache()` which is going to compare all the transaction inputs to our mempool, checking that they have not already been marked as spent.
If the coin is not already spent, we reference it from either the UTXO set or our mempool, and finally submit it through `CheckInputScripts()` once more, this time caching both the signatures and the full scripts.

.Click to show `CheckInputsFromMempoolAndCache()`
[%collapsible]
====
.src/validation.cpp#CheckInputsFromMempoolAndCache
[source,cpp,options=nowrap]
----
/**
* Checks to avoid mempool polluting consensus critical paths since cached
* signature and script validity results will be reused if we validate this
* transaction again during block validation.
* */
static bool CheckInputsFromMempoolAndCache(const CTransaction& tx, TxValidationState& state,
                const CCoinsViewCache& view, const CTxMemPool& pool,
                unsigned int flags, PrecomputedTransactionData& txdata, CCoinsViewCache& coins_tip)
                EXCLUSIVE_LOCKS_REQUIRED(cs_main, pool.cs)
{
    AssertLockHeld(cs_main);
    AssertLockHeld(pool.cs);

    assert(!tx.IsCoinBase());
    for (const CTxIn& txin : tx.vin) {
        const Coin& coin = view.AccessCoin(txin.prevout);

        // This coin was checked in PreChecks and MemPoolAccept
        // has been holding cs_main since then.
        Assume(!coin.IsSpent());
        if (coin.IsSpent()) return false;

        // If the Coin is available, there are 2 possibilities:
        // it is available in our current ChainstateActive UTXO set,
        // or it's a UTXO provided by a transaction in our mempool.
        // Ensure the scriptPubKeys in Coins from CoinsView are correct.
        const CTransactionRef& txFrom = pool.get(txin.prevout.hash);
        if (txFrom) {
            assert(txFrom->GetHash() == txin.prevout.hash);
            assert(txFrom->vout.size() > txin.prevout.n);
            assert(txFrom->vout[txin.prevout.n] == coin.out);
        } else {
            assert(std::addressof(::ChainstateActive().CoinsTip()) == std::addressof(coins_tip));
            const Coin& coinFromUTXOSet = coins_tip.AccessCoin(txin.prevout);
            assert(!coinFromUTXOSet.IsSpent());
            assert(coinFromUTXOSet.out == coin.out);
        }
    }

    // Call CheckInputScripts() to cache signature and script validity against current tip consensus rules.
    return CheckInputScripts(tx, state, view, flags, /* cacheSigStore = */ true, /* cacheFullSciptStore = */ true, txdata);
}
----
====

=== PackageMempoolChecks

`PackageMempoolChecks` are designed to "Enforce package mempool ancestor/descendant limits (distinct from individual ancestor/descendant limits done in PreChecks)".
They take a vector of ``CTransactionRef``s and a `PackageValidationState`.

Again we take <<multiple_locks,two locks>> before checking that the transactions are not in the mempool.
Any transactions which are part of the package and were in the mempool will have already been removed by `MemPoolAccept::AcceptPackage()`.

Finally we check the package limits, which consists of checking the {ancestor|descendant} {count|size}.
This check is unique to packages because we can now add descendants whose ancestors would not otherwise qualify for entry into our mempool with their low effective fee rate.

=== Finalize

Provided that consensus script checks pass and this was not a test ATMP call, we will call `Finalize()` on the transaction.
This will remove any conflicting (lower fee) transactions from the mempool before adding this one, finishing by trimming the mempool to the configured size (default: `static const unsigned int DEFAULT_MAX_MEMPOOL_SIZE = 300;` MB).
In the event that *this* transaction got trimmed, we ensure that we return a `TxValidationResult::TX_MEMPOOL_POLICY, "mempool full"` result.

=== Transactions from blocks

Transactions learned about from blocks:

* Might not be present in our mempool
* Are not being considered for entry into our mempool and therefore do not have to pass policy checks
* Are only subject to consensus checks

This means that we can validate these transactions based only on our copy of the UTXO set and the data contained within the block itself.
We call `ProcessBlock()` when processing new blocks received from the P2P network (in _net_processing.cpp_) from net message types: `NetMsgType::CMPCTBLOCK`, `NetMsgType::BLOCKTXN` and `NetMsgType::BLOCK`.


.Abbreviated block transaction validation
[mermaid,target=block-tx-validation,format=svg,id=block-tx-validation]
....
flowchart LR
    process_block["ProcessBlock()"]
    process_new_block["ProcessNewBlock()"]
    check_block_header["CheckBlockHeader()"]
    block_merkle["BlockMerkleRoot()"]
    check_transaction["CheckTransaction()"]
    subgraph sub_check_block ["CheckBlock()"]
        direction TB
        check_block_header --> block_merkle
        block_merkle --> check_transaction
    end

    accept_block_header["AcceptBlockHeader()"]
    check_block_index["CheckBlockIndex()"]
    check_block["CheckBlock()"]
    contextual_check_block["ContextualCheckBlock()"]
    save_block_disk["SaveBlockToDisk()"]
    recv_block_tx["ReceivedBlockTransactions()"]
    subgraph sub_accept_block["AcceptBlock()"]
        direction TB
        accept_block_header --> check_block_index
        check_block_index --> check_block
        check_block --> contextual_check_block
        contextual_check_block --> save_block_disk
        save_block_disk --> recv_block_tx
    end

    activate_best_chain_step["ActivateBestChainStep()"]
    connect_tip["ConnectTip()"]
    connect_block["ConnectBlock()"]

    subgraph activate_chain["ActivateBestChain()"]
        direction TB
        activate_best_chain_step --> connect_tip
        connect_tip --> connect_block
    end

    process_block --> process_new_block
    process_new_block --> sub_check_block
    sub_check_block --> sub_accept_block
    sub_accept_block --> activate_chain
....

The general flow of `ProcessBlock()` is that will call `CheckBlock()`, `AcceptBlock()` and then `ActivateBestChain()`.
A block which has passed successfully through `CheckBlock()` and `AcceptBlock()` has *not* passed full consensus validation.

`CheckBlock()` does some cheap, context-independent structural validity https://github.com/bitcoin/bitcoin/blob/v23.0/src/validation.cpp#L3242-L3314[checks^], along with (re-)checking the proof of work in the header, however these checks just determine that the block is "valid-enough" to proceed to `AcceptBlock()`.

Once the checks have been completed, the `block.fChecked` value is set to `true`.
This will enable any subsequent calls to this function _with this block_ to be skipped.

`AcceptBlock()` is used to persist the block to disk so that we can (validate it and) add it to our chain immediately, use it later, or discard it later.
`AcceptBlock()` makes a second call to `CheckBlock()` but because `block.fChecked` was set to `true` on the first pass this second check will be skipped.

TIP: `AcceptBlock()` contains an inner call to `CheckBlock()` because it can also be called directly by `CChainState::LoadExternalBlockFile()` where `CheckBlock()` will not have been previously called.

It also now runs some https://github.com/bitcoin/bitcoin/blob/v23.0/src/validation.cpp#L3662-L3663[contextual checks^] such as checking the block time, transaction lock times (transaction are "finalized") and witness commitments are either non-existent or valid (https://github.com/bitcoin/bitcoin/blob/v23.0/src/validation.cpp#L3412-L3492[link^]).
After this the block will be serialized to disk.

[NOTE]
====
At this stage we might still be writing blocks to disk that will fail full consensus checks.
However, if they have reached here they have passed proof of work and structural checks, so consensus failures may be due to us missing intermediate blocks, or that there are competing chain tips.
In these cases this block may still be useful to us in the future.
====

Once the block has been written to disk by `AcceptBlock()` full validation of the block and its transactions begins via `CChainState::ActivateBestChain()` and its inner call to `ActivateBestChainStep()`.

////
As part of `ProcessBlock()` we end up calling `CheckBlock()` twice: once on the inner `ProcessNewBlock()` and, if this first is successful, once again inside of `AcceptBlock()`.
We find the following code comment inside `ProcessBlock()`:

._validation.cpp#ChainstateManager::ProcessNewBlock()_
[source,cpp,options=nowrap]
----
    // Skipping AcceptBlock() for CheckBlock() failures means that we will never mark a block as invalid if
    // CheckBlock() fails.  This is protective against consensus failure if there are any unknown forms of block
    // malleability that cause CheckBlock() to fail; see e.g. CVE-2012-2459 and
    // https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-February/016697.html.  Because CheckBlock() is
    // not very expensive, the anti-DoS benefits of caching failure (of a definitely-invalid block) are not substantial.
    bool ret = CheckBlock(*block, state, chainparams.GetConsensus());
    if (ret) {
        // Store to disk
        ret = ActiveChainstate().AcceptBlock(block, state, &pindex, force_processing, nullptr, new_block);
    }
----

The threat vector being addressed is that a malicious node could create a block (with malleated merkle tree interior) but still have it compute the same merkle root.
This would lead to nodes marking this block as invalid as expected.
However, a valid un-malleated block **with the same merkle root**, which we might receive later from an honest peer, would be rejected by our node because we cache "bad" blocks using the `m_blockman.m_dirty_blockindex` set:

._validation.cpp#CChainState::AcceptBlock()_
[source,cpp,highlight=5,options=nowrap]
----
    if (!CheckBlock(block, state, m_params.GetConsensus()) ||
        !ContextualCheckBlock(block, state, m_params.GetConsensus(), pindex->pprev)) {
        if (state.IsInvalid() && state.GetResult() != BlockValidationResult::BLOCK_MUTATED) {
            pindex->nStatus |= BLOCK_FAILED_VALID;
            m_blockman.m_dirty_blockindex.insert(pindex);
        }
        return error("%s: %s", __func__, state.ToString());
    }
----

The rationale for caching bad blocks is so that we don't expend resources re-validating and propagating them, opening ourselves and the wider network up to a DoS vector, where an attacker can flood nodes with invalid blocks and hope they expend resources gossiping and re-validating them.

Therefore we call `CheckBlock()` first, and only try `AcceptBlock()` if this passes.

Note here how the developers have had to balance consideration for sensitive validation code, staying in consensus with the rest of the network and avoiding potential P2P DoS attacks.
This type of thinking is common across the codebase.
////

////
TODO: Note that the primary validation occurs inside `ConnectBlock()`, not `CheckBlock()` and `AcceptBlock()`
////

== Multiple chains

TODO: Reorgs, undo data, `DisconnectBlock`

Bitcoin nodes should ultimately converge in consensus on the most-work chain.
Being able to track and monitor multiple chain (tips) concurrently is a key requirement for this to take place.
There are a number of different states which the client must be able to handle:

. A single, most-work chain being followed
. Stale blocks learned about but not used
. Full reorganisation from one chain tip to another

`BlockManager` is tasked with maintaining a tree of all blocks learned about, along with their total work so that the most-work chain can be quickly determined.

`CChainstate` (https://github.com/bitcoin/bitcoin/pull/24513[renamed^] to `Chainstate` in v24.0) is responsible for updating our local view of the best tip, including reading and writing blocks to disk, and updating the UTXO set.
A single `BlockManager` is shared between all instances of `CChainState`.

`ChainstateManager` is tasked with managing multiple ``CChainState``s.
Currently just a "regular" IBD chainstate and an optional snapshot chainstate, which might in the future be used as part of the https://bitcoinops.org/en/topics/assumeutxo/[assumeUTXO^] project.

When a new block is learned about (from `src/net_processing.cpp`) it will call into ``ChainstateManager``s `ProcessNewBlockHeaders` method to validate it.

== Responsible Disclosure

Bitcoin Core has a defined process for reporting security vulnerabilities via it's responsible disclosure process.
This is detailed in https://github.com/bitcoin/bitcoin/blob/master/SECURITY.md[SECURITY.md^].

Bugs which would need to be disclosed by following this process are generally those which could result in a consensus-failure, theft of funds, or creation of additional supply tokens (new coin issuance).
If bugs of this nature are posted publicly then inevitably one or more persons will try to enact them, possibly causing severe harm or loss to one or many people.

If you would like to learn more about the responsible disclosure process and why it's so important for Bitcoin Core, you can read the following:

. https://medium.com/mit-media-lab-digital-currency-initiative/http-coryfields-com-cash-48a99b85aad4[Responsible disclosure in the era of cryptocurrencies^]
. https://cacm.acm.org/magazines/2020/10/247597-responsible-vulnerability-disclosure-in-cryptocurrencies/fulltext[Responsible Vulnerability Disclosure in Cryptocurrencies^]

== Exercises

[qanda]
What is the difference between contextual and context-free validation checks?::
Contextual checks require some knowledge of the current "state", e.g. ChainState, chain tip or UTXO set.
+
Context-free checks only require the information required in the transaction itself.
+
See {glozow-tx-mempool-validation}[glozow-tx-mempool-validation] for more info.

What are some examples of each?::
context-free:
+
. `tx.isCoinbase()`
. https://github.com/bitcoin/bitcoin/tree/4b5659c6b115315c9fd2902b4edd4b960a5e066e/src/consensus/tx_check.cpp#L25-L28[0 &#8804; tx_value &#8804; MAX_MONEY^]
. https://github.com/bitcoin/bitcoin/tree/4b5659c6b115315c9fd2902b4edd4b960a5e066e/src/policy/policy.cpp#L88[tx not overweight^]

+
contextual: https://github.com/bitcoin/bitcoin/tree/4b5659c6b115315c9fd2902b4edd4b960a5e066e/src/validation.cpp#L671-L692[check inputs are available^]

In which function(s) do UTXO-related validity checks happen?::
`ConnectBlock()`

What type of validation checks are `CheckBlockHeader()` and `CheckBlock()` performing?::
context-free

Which class is in charge of managing the current blockchain?::
`ChainstateManager()`

Which class is in charge of managing the UTXO set?::
`CCoinsViews()`

Which functions are called when a longer chain is found that we need to re-org onto?::
TODO

Are there any areas of the codebase where the same consensus or validation checks are performed twice?::
Again see https://github.com/glozow/bitcoin-notes/tree/e9855dc377811b6d77bb75d8606c776cc26c1860/transaction-lifecycle.md#Validation-and-Submission-to-Mempool[glozows notes^] for examples

Why does `CheckInputsFromMempoolAndCache` exist?::
To prevent us from re-checking the scripts of transactions already in our mempool during consensus validation on learning about a new block

Which function(s) are in charge of validating the merkle root of a block?::
`BlockMerkleRoot()` and `BlockWitnessMerkleRoot()` construct a vector of merkle leaves, which is then passed to `ComputeMerkleRoot()` for calculation.
// TODO: Calculate the merkle root of a sample block

Can you find any evidence (e.g. PRs) which have been made in an effort to modularize consensus code?::
A few examples: https://github.com/bitcoin/bitcoin/pull/10279[PR#10279^], https://github.com/bitcoin/bitcoin/pull/20158[PR#20158^]

What is the function of `BlockManager()`?::
It manages the current most-work chaintip and pruning of unneeded blocks (`\*.blk`) and associated undo (`*.rev`) files

What stops a malicious node from sending multiple invalid headers to try and use up a nodes' disk space? (hint: these might be stored in `BlockManager.m_failed_blocks`)::
Even invalid headers would need a valid proof of work which would be too costly to construct for a spammer

Which functions are responsible for writing consensus-valid blocks to disk?::
TODO: answer

Are there any other components to Bitcoin Core which, similarly to the block storage database, are not themselves performing validation but can still be consensus-critical?::
Not sure myself, sounds like an interesting question though!

In which module (and class) is signature verification handled?::
`src/script/interpreter.cpp#BaseSignatureChecker`

Which function is used to calculate the Merkle root of a block, and from where is it called?::
`src/consensus/merkle.cpp#ComputeMerkleRoot` is used to compute the merkle root.
+
It is called from `src/chainparams.cpp#CreateGenesisBlock`, `src/miner.cpp#IncrementExtraNonce` & `src/miner.cpp#RegenerateCommitments` and from `src/validation.cpp#CheckBlock` to validate incoming blocks.

Practical question on Merkle root calculation::
TODO, add more Exercises

* Modify the https://github.com/bitcoin/bitcoin/blob/v23.0/src/script/script.h#L444-L450[code^] which is used to add new opcodes to a `CScript` without breaking consensus.

// == Removed text
//
// The outline of the mechanism at work is that a node relaying a transaction can slightly modify the signature in a way which is still acceptable to the underlying OpenSSL module.
// Once the signature has been changed, the transaction ID (hash) will also change.
// If the modified transaction is then included in a block, before the original, the effect is that the sender will still see the outgoing transaction as "unconfirmed" in their wallet.
// The sender wallet should however also see the accepted (modified) outgoing transaction, so their balance will be calculated correctly, only a "stuck doublespend" will pollute their wallet.
// The receiver will not perceive anything unordinary, unless they were tracking the incoming payment using the txid as given to them by the sender.
